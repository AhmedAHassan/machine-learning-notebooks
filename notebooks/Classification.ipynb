{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Cross-Validation\n",
    "\n",
    "**Instructions:**\n",
    "* go through the notebook and complete the **tasks** .  \n",
    "* Make sure you understand the examples given!\n",
    "* When a question allows a free-form answer (e.g., ``what do you observe?``) create a new markdown cell below and answer the question in the notebook.\n",
    "* ** Save your notebooks when you are done! **\n",
    "\n",
    "In the previous lab, we loaded up the iris dataset for flower classification, and performed simple exploratory data analysis, i.e., we visualized the data available (features given class labels) in order to understand characteristics of the data (e.g., that some classes are easier to be separated from others based on some features, etc.)\n",
    "\n",
    "If you don't remember much about this, please revisit the corresponding lab (Lab 2) before moving on.\n",
    "\n",
    "In this lab, we will go through the process of actually training a classifier on a dataset (training set), and evaluating the performance of the classifier on unknown data (test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Run the cell below to load our data. Notice the last line, where we add some random Gaussian noise to our data to make the task more challenging (data in real life usually contains some form of noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#view a description of the dataset (uncomment next line to do so)\n",
    "print(iris.DESCR)\n",
    "\n",
    "#Set X a samples times features matrix, Y equal to the targets\n",
    "X=iris.data \n",
    "y=iris.target \n",
    "\n",
    "\n",
    "#we add some random noise to our data to make the task more challenging\n",
    "X=X+np.random.normal(0,0.4,X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> How many data samples do we have?  Print the value below using ``shape`` on X appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = X.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> How many features do we have?  Print the value below using ``shape`` on X appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_features = X.shape[1]\n",
    "number_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> How many classes do we have?  Print the value below using ``np.unique`` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> How many samples do we have that belong to class 1?  Use the ``np.where`` function appropriately on y to print this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[np.where(y==1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Assume we want to generate a list of shuffled indices of our data.  Use the function ``numpy.random.permute`` to do that.  In the cell below, you can already see how to create a list of indices that is **not** shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([135,   9, 124, 125,  21, 137, 133,  64,  82,  62,  13, 114,   2,\n",
       "       144,  76,  87,  48,  43,  73, 139,  33,  89,  20, 110,  31,  44,\n",
       "       103,  67, 108,  32, 105,  75,  70, 121,   1, 117, 123, 136,  96,\n",
       "        27,  55,   8, 102,   0,  14, 101,  16,  45,  68, 112,  86,  56,\n",
       "       141, 128,  47,  19,  17,  57,  12, 145,  26, 127,  84,  71,  94,\n",
       "        29,  60,   7,  49,  58,  15,  91, 115,  37, 113,  41,  95,  46,\n",
       "        38,  72,  97,  36,  11,  81,  63,  39,  24,   4,  66,  22,  25,\n",
       "        18, 119, 120,  77,  88, 143,  90, 132,  85, 107, 129,  61, 100,\n",
       "        23,  83,  34,  98,  74,  79, 131, 109,  30, 147,  10, 148, 142,\n",
       "        65, 118,   5,  54,  80, 122,  52, 116,  92,  42, 134,  28, 140,\n",
       "       138,  53, 146, 111,  69, 130,  78,  51,  50, 126, 106, 104,  99,\n",
       "        59, 149,  93,  40,  35,   6,   3])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=list(range(X.shape[0]))\n",
    "print(L)\n",
    "#Enter code here\n",
    "shuffled = np.random.permutation(L)\n",
    "shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Here is an example of using the k-NN classifier.  We split our data to training and testing (with a 0.2 percentage for our test data), fit on the training data, test on the testing data.  Go through the code and make sure you understand it.  Subsequently, do the same for the next cell, that prints the confusion matrix and the total accuracy.  (documentation: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "**note: for this lab, we use the euclidean distance along with 10 neighbours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 0 2 2 0 0 2 2 2 1 0 0 2 1 1 0 2 1 2 0 0 2 1 0 0 1 0 0]\n",
      "[2 0 2 0 2 1 0 0 2 2 1 1 0 0 1 1 1 0 2 1 2 0 0 2 1 0 0 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntwice daily oil\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#define knn classifier, with 5 neighbors and use the euclidian distance\n",
    "knn=KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "#define training and testing data, fit the classifier\n",
    "knn.fit(X_train,y_train)\n",
    "#print(X_train)\n",
    "#predict values for test data based on training data\n",
    "#print(X_test)\n",
    "y_pred=knn.predict(X_test)\n",
    "#print values\n",
    "print(y_test) # true values\n",
    "print(y_pred) # predicted values\n",
    "\n",
    "\"\"\"\n",
    "twice daily oil\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  1 11]]\n",
      "overall accuracy: 0.9666666666666667\n",
      "class precision: [1.         0.92307692 1.        ]\n",
      "class recall: [1.         1.         0.91666667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('overall accuracy: %s' % accuracy_score(y_test,y_pred))\n",
    "\n",
    "# we can also generate the class-relative precision and recall \n",
    "print('class precision: %s' % precision_score(y_test,y_pred,average=None))\n",
    "print('class recall: %s' %recall_score(y_test,y_pred,average=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Write your **own** functions that return the confusion matrix given the true and predicted labels, as well as the accuracy.  To do so, fill in the code in the next two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 0, 0], [0, 12, 0], [0, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "#create a matrix with entries equal to zero, and subsequently build the confusion matrix\n",
    "#the method should return the confusion matrix in a numpy array\n",
    "def myConfMat(y_test,y_pred,classno):\n",
    "    C = [[0, 0, 0], # initialize the confusion matrix to zeros\n",
    "         [0, 0, 0],\n",
    "         [0, 0, 0]] \n",
    "    \n",
    "    for i in range(0, len(y_pred)):\n",
    "        C[y_test[i]][y_pred[i]] +=1\n",
    "    #loop through all results and update the confusion matrix\n",
    "    return C\n",
    "\n",
    "#note: len(np.unique(y))  indicates the dimensions of the confusion matrix (why?)\n",
    "print(myConfMat(y_test,y_pred,len(np.unique(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "#use the numpy function where to return the accuracy given the true/predicted labels.  i.e., #correct/#total\n",
    "def myAccuracy(y_test,y_pred):\n",
    "#     print(y_test)\n",
    "#     print(y_pred)\n",
    "#     print(\"\\n\")\n",
    "    accuracy = len(y_test[np.where(y_test==y_pred)])/len(y_pred) #np.where()\n",
    "    return accuracy\n",
    "    \n",
    "print(myAccuracy(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Write your own cross-validation function.  In this case, we are using a fixed distance (euclidean) and a fixed number of neighbours (10) so we do **not** need to create a validation set.\n",
    "\n",
    "Your function (see cell below) firstly splits the indices of each of our data into bins according to the number of folds (here: 5-fold).\n",
    "\n",
    "Then, you should loop through all folds, split the data into training and testing by selecting the appropriate bins (see slides on cross-validation), train on training data and save the test result as the accuracy for each fold (see list accuracy_fold).  This is the list that your function should return in the end.  Remember that the ``extend`` function extends a list with more values.  \n",
    "\n",
    "The final print call in the end of the cell should print the list of accuracies, with five values, one for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9, 0.9, 0.9, 0.8333333333333334, 0.9666666666666667]\n"
     ]
    }
   ],
   "source": [
    "def myCrossVal(X,y,foldK):\n",
    "    accuracy_fold=[] #list to store accuracies folds \n",
    "    #TASK: use the function np.random.permutation to generate a list of shuffled indices \n",
    "    #from in the range (0,number of data)\n",
    "    #(you did this already in a task above)\n",
    "    L=list(range(X.shape[0]))\n",
    "    indices = np.random.permutation(L)\n",
    "    #print(indices)\n",
    "    \n",
    "    #TASK: use the function array_split to split the indices to k different bins:\n",
    "    #uncomment line below\n",
    "    bins=np.array(np.array_split(indices, foldK))\n",
    "    #print(bins)\n",
    "    \n",
    "    #loop through folds\n",
    "    for i in range(0,foldK):\n",
    "        foldTrain=[] # list to save current indices for training\n",
    "        foldTest=[]  # list to save current indices for testing\n",
    "        #TASK: take bin i for testing, rest for training.  \n",
    "        #Can use the function extend to add indices to foldTrain and foldTest\n",
    "        #train kNN classifier\n",
    "        foldTest.extend(bins[i])\n",
    "        for j in range(0, foldK):\n",
    "            if j!=i:\n",
    "                foldTrain.extend(bins[j])\n",
    "        #print(foldTrain)\n",
    "        knn=KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "        knn.fit(X[foldTrain],y[foldTrain])\n",
    "        #test on test data\n",
    "        y_pred=knn.predict(X[foldTest])\n",
    "        #append the new accuracy to your accuracy_fold list.  \n",
    "        #You can use accuracy_score or your myAccuracy function.\n",
    "        accuracy_fold.append(myAccuracy(y[foldTest], y_pred))\n",
    "    return accuracy_fold;\n",
    "    \n",
    "accuracy_fold=myCrossVal(X,y,5)\n",
    "print(accuracy_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Task:**</span> Print the average accuracy and standard deviation of your results over the 5 folds. (functions ``mean`` and ``std``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy: 0.9\n",
      "standard deviation: 0.07601169500660919\n"
     ]
    }
   ],
   "source": [
    "print(\"mean accuracy: %s\" % np.mean(accuracy_fold))\n",
    "print(\"standard deviation: %s\" % np.std(accuracy_fold))\n",
    "#np.std?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<span style=\"color:rgb(170,0,0)\">**Optional task:**</span> Write your own functions to calculate class-relative precision and recall. Compare these to the sklearn functions ``precision_score`` and ``recall_score`` that were used above on the original y_test and y_pred values (from the beginning of this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homemade functions:\n",
      "classes:      [0 1 2] \n",
      "my precision: [1.         0.76923077 1.        ]\n",
      "my recall:    [1.   1.   0.75] \n",
      "\n",
      "\n",
      "functions from sklearn:\n",
      "class precision: [1.         0.76923077 1.        ]\n",
      "class recall: [1.   1.   0.75]\n"
     ]
    }
   ],
   "source": [
    "#hint: you can use the output from your myConfMat function above\n",
    "\n",
    "def myPrecision(y_test,y_pred):\n",
    "    classes = np.unique(y_pred)\n",
    "    precision = np.zeros(classes.shape)\n",
    "    # get confusion matrix\n",
    "    conf_matrix = myConfMat(y_test,y_pred,len(np.unique(y)))\n",
    "    for i in range(0,3):\n",
    "        tp_plus_fp=0\n",
    "        for j in classes:\n",
    "            # true positives + false positives\n",
    "            tp_plus_fp+=conf_matrix[j][i]\n",
    "        # precision = true positives / true positives + false positives\n",
    "        class_precision=conf_matrix[i][i]/tp_plus_fp\n",
    "        precision[i]=class_precision\n",
    "    return precision\n",
    "\n",
    "def myRecall(y_test,y_pred):\n",
    "    classes = np.unique(y_pred)\n",
    "    recall = np.zeros(classes.shape)\n",
    "    # get confusion matrix\n",
    "    conf_matrix = myConfMat(y_test,y_pred,len(np.unique(y)))\n",
    "    for i in range(0,3):\n",
    "        tp_plus_fn=0\n",
    "        for j in classes:\n",
    "            # true positives + false negatives\n",
    "            tp_plus_fn+=conf_matrix[i][j]\n",
    "        # recall = true positives / true positives + false negatives\n",
    "        class_recall=conf_matrix[i][i]/tp_plus_fn\n",
    "        recall[i]=class_recall\n",
    "    return recall\n",
    "\n",
    "print(\"homemade functions:\")\n",
    "print('classes:      %s ' % np.unique(y_pred) )    \n",
    "print('my precision: %s' % myPrecision(y_test,y_pred))\n",
    "print('my recall:    %s \\n\\n' % myRecall(y_test,y_pred))\n",
    "\n",
    "print(\"functions from sklearn:\")\n",
    "print('class precision: %s' % precision_score(y_test,y_pred,average=None))\n",
    "print('class recall: %s' %recall_score(y_test,y_pred,average=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
